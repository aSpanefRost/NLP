{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18CS30034_Assn1_NLP_A21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z_wN2v1RT1F"
      },
      "source": [
        "# **Assignment-1 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 4th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \n",
        "\n",
        "#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a35tmEySCx7"
      },
      "source": [
        "### **Submission Details:**\n",
        "Name: Rajdeep Das\n",
        "\n",
        "Roll No.: 18CS30034\n",
        "\n",
        "Department: Computer Science and Engineering\n",
        "\n",
        "Email-ID: rajdeepdasren@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weHMmyd8fnq"
      },
      "source": [
        "## **Reading a Raw Text Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSy_LOK2aGQ"
      },
      "source": [
        "Retrieve & save raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rku6rV2ORpZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3a265e-5280-4fb3-f444-2e519f5972f0"
      },
      "source": [
        "# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\n",
        "# both inclusive, from the link below:\n",
        "# \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "# Save this corpus in a text file, named as 'rawCorpus.txt'\n",
        "# Print the total number of characters in the text file \n",
        "\n",
        "# *** Write code ***\n",
        "\n",
        "##start\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "  \n",
        "# here we have to pass url and path\n",
        "# (where you want to save ur text file)\n",
        "urllib.request.urlretrieve(\"https://www.gutenberg.org/files/730/730-0.txt\",\n",
        "                           \"/content/sample_data/temp.txt\")\n",
        "  \n",
        "file = open(\"/content/sample_data/temp.txt\", \"r\")\n",
        "contents = file.read()\n",
        "soup = BeautifulSoup(contents, 'html.parser')\n",
        " \n",
        "\n",
        "textt=soup.get_text()\n",
        "textt=textt.split(\"CHAPTER I\",1)[1].split(\"CHAPTER XI\")[0]\n",
        "textt=\"CHAPTER I\"+textt\n",
        "textt=textt.strip()\n",
        "\n",
        "print(\"Total number of characters in the text file: \",len(textt))\n",
        "\n",
        "f = open(\"rawCorpus.txt\", \"w\")\n",
        "f.writelines(textt)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters in the text file:  148711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZIOy0Y2hzQ"
      },
      "source": [
        "Read the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsdBJa_l2l7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cb5745-bb60-4489-d982-fb2c42ebb348"
      },
      "source": [
        "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
        "# *** Write code ***\n",
        "f=open(\"rawCorpus.txt\",\"r\")\n",
        "rawReadCorpus=f.read()\n",
        "\n",
        "print (\"Total # of characters in read dataset: {}\".format(len(rawReadCorpus)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # of characters in read dataset: 148711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhkmGsSoV0zG"
      },
      "source": [
        "## **Installing NLTK**\n",
        "\n",
        "The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \n",
        "\n",
        "Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\n",
        "\n",
        "To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\n",
        "```bash\n",
        "conda create -n myenv python=3.6\n",
        "conda activate myenv\n",
        "```\n",
        "\n",
        "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\n",
        "```bash\n",
        "sudo pip3 install nltk \n",
        "python3 \n",
        "nltk.download()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKtZeHq4N98"
      },
      "source": [
        "## **Preprocessing the corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LSUX__82Ff"
      },
      "source": [
        "**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \"\\_will\\_\" (that should ideally appear as \"will\"), \"wouldn't\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7eO4Dm4jIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e194ef8-aa19-4675-8f58-bb89065d04bc"
      },
      "source": [
        "# Importing modules\n",
        "!pip install nltk==3.6.2\n",
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 36.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 184 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 276 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 317 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 358 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 34.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (4.62.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nltk version is 3.6.2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWIzYXyz9Zt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd4b1fd-b779-4dc4-afe0-015f1ea2cdbe"
      },
      "source": [
        "import string\n",
        "# *** Write code for preprocessing the corpus ***\n",
        "#rawReadCorpus=\"\".join([i.lower() for i in rawReadCorpus if i not in string.punctuation])\n",
        "\n",
        "def filter(text):\n",
        "   new_text=\"\".join([i.lower() for i in text if i not in string.punctuation])\n",
        "   return new_text\n",
        "\n",
        "#print(rawReadCorpus)\n",
        "sentences=sent_tokenize(rawReadCorpus)\n",
        "#print(sent_tokenize(rawReadCorpus))\n",
        "#print(sentences)\n",
        "new_sentences = []\n",
        "for i in sentences:\n",
        "  i=i.replace('\\n',' ')\n",
        "  i=i.replace('”','')\n",
        "  i=i.replace('“','')\n",
        "  # i=i.replace('’','')\n",
        "  new_sentences.append(filter(i))\n",
        "\n",
        "#print(new_sentences)\n",
        "\n",
        "words = []\n",
        "for i in new_sentences:\n",
        "  for x in word_tokenize(i):\n",
        "           words.append(x)\n",
        "\n",
        "#print(len(words))\n",
        "# words=word_tokenize(rawReadCorpus)\n",
        "# # Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
        "\n",
        "print(\"First 5 sentences::: \\n\")\n",
        "for line in new_sentences[0:5]:\n",
        "    print(line)\n",
        "    print(\"\\n\")\n",
        "   \n",
        "print(\"First 5 words::: \\n\")   \n",
        "#Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
        "for word in words[:5]:\n",
        "    print(word)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences::: \n",
            "\n",
            "chapter i\n",
            "\n",
            "\n",
            "treats of the place where oliver twist was born and of the circumstances attending his birth   among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter\n",
            "\n",
            "\n",
            "for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country\n",
            "\n",
            "\n",
            "although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred\n",
            "\n",
            "\n",
            "the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respiration—a troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter\n",
            "\n",
            "\n",
            "First 5 words::: \n",
            "\n",
            "chapter\n",
            "i\n",
            "treats\n",
            "of\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ75_a1QL70J"
      },
      "source": [
        "**Perform the following tasks for the given corpus:**\n",
        "1. Print the average number of tokens per sentence.\n",
        "2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
        "3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyG0g3oSADmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2cdc25d-040f-4f69-f1eb-06b581543573"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydHIxC7lG7Py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d9869a-f1a1-4b5b-c98c-77294124c860"
      },
      "source": [
        "# *** Write code for the 2 tasks above ***\n",
        "total_tokens=0\n",
        "total_sentences=len(new_sentences)\n",
        "for x in new_sentences:\n",
        "    total_tokens=total_tokens+len(x.split())\n",
        "#print(total_tokens,total_sentences)\n",
        "print(\"Average number of tokens per sentence: \",total_tokens/total_sentences)\n",
        "\n",
        "\n",
        "min_len=0\n",
        "max_len=0\n",
        "val=0\n",
        "\n",
        "for x in new_sentences:\n",
        "    val=val+word_tokenize(x).count('oliver')\n",
        "    if 'oliver' in x:\n",
        "        #print(val)\n",
        "        x_len=len(x.split())\n",
        "        if(x_len>max_len):\n",
        "          max_len=x_len\n",
        "        if(min_len==0):\n",
        "          min_len=x_len\n",
        "        if(x_len<min_len):\n",
        "          min_len=x_len\n",
        "\n",
        "print(\"Length of smallest and longest sentence that contains 'Oliver' respectively: \",min_len,max_len)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(\"Number of unique token in the corpus, after stop word removal: \",len(set(words)))\n",
        "#print(val)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of tokens per sentence:  23.794520547945204\n",
            "Length of smallest and longest sentence that contains 'Oliver' respectively:  2 114\n",
            "Number of unique token in the corpus, after stop word removal:  4148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RiDR7TJjKX"
      },
      "source": [
        "## **Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJeTSt8HM95L"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the given corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "\n",
        "2. **List the top 10 bigrams, trigrams**\n",
        "(Additionally remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPXGvVaR-ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26f0303-9805-473d-e2fa-68d2cbe8e256"
      },
      "source": [
        "\n",
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "\n",
        "for content in new_sentences: # *** Write code ***\n",
        "    unigrams.extend(ngrams(word_tokenize(content),1))\n",
        "    #bigrams.extend(ngrams(content.split(),2))\n",
        "    bigrams.extend(ngrams(word_tokenize(content),2))\n",
        "    trigrams.extend(ngrams(word_tokenize(content),3))\n",
        "    ##similar for trigrams \n",
        "    # *** Write code ***\n",
        "   # words = [w for w in words if not w in stop_words]\n",
        "   \n",
        "unigrams= [ word for word in unigrams if not any(x in word for x in ['’'])]\n",
        "bigrams= [ word for word in bigrams if not any(x in word for x in ['’'])]\n",
        "trigrams= [ word for word in trigrams if not any(x in word for x in ['’'])]\n",
        "\n",
        "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams[:5]) + \" ...\\n\")\n",
        "\n",
        "# list of unigram, bigram & trigram after removing those that \n",
        "# totally contain only articles, prepositions, determiners\n",
        "# Eg. For bigrams, don't remove items like (\"a\", \"boy\") --> where not all are \n",
        "#     articles, prepositions, determiners\n",
        "#     But remove items like (\"in\", \"the\") --> where all are articles, prepositions, determiners\n",
        "\n",
        "# Similarly, for unigrams and trigrams\n",
        "\n",
        "#removes ngrams containing only stopwords\n",
        "def removal(x):     \n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "\n",
        "unigrams_Processed = removal(unigrams)\n",
        "bigrams_Processed = removal(bigrams)\n",
        "trigrams_Processed = removal(trigrams)\n",
        "\n",
        "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
        "\n",
        "def get_ngrams_freqDist(n, ngramList):\n",
        "    #This function computes the frequency corresponding to each ngram in ngramList \n",
        "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
        "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
        "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
        "    \n",
        "    # *** Write code ***\n",
        "    ngram_freq_dict=nltk.FreqDist(ngramList)\n",
        "    \n",
        "    return ngram_freq_dict\n",
        "\n",
        "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
        "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
        "bigrams_freqDist = get_ngrams_freqDist(2, bigrams)\n",
        "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "trigrams_freqDist = get_ngrams_freqDist(3, trigrams)\n",
        "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)                                                 \n",
        "\n",
        "# # Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
        "# # *** Write code ***\n",
        "print(\"Top 10 unigrams, having highest frequency as in unigrams_freqDist:\")\n",
        "print(unigrams_freqDist.most_common(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# # Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
        "# # *** Write code ***\n",
        "print(\"Top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist:\")\n",
        "print(unigrams_Processed_freqDist.most_common(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# # Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
        "# # *** Write code ***\n",
        "print(\"Top 10 bigrams, having highest frequency as in bigrams_freqDist:\")\n",
        "print(bigrams_freqDist.most_common(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# # Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
        "# # *** Write code ***\n",
        "print(\"Top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist:\")\n",
        "print(bigrams_Processed_freqDist.most_common(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# # Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
        "# # *** Write code ***\n",
        "print(\"Top 10 trigrams, having highest frequency as in trigrams_freqDist:\")\n",
        "print(trigrams_freqDist.most_common(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# # Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
        "# # *** Write code ***\n",
        "print(\"Top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist:\")\n",
        "print(trigrams_Processed_freqDist.most_common(10))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of n-grams:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "[('chapter',), ('i',), ('treats',), ('of',), ('the',)] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('chapter', 'i'), ('treats', 'of'), ('of', 'the'), ('the', 'place'), ('place', 'where')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "Sample of n-grams after processing:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "[('chapter',), ('treats',), ('place',), ('oliver',), ('twist',)] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('chapter', 'i'), ('treats', 'of'), ('the', 'place'), ('place', 'where'), ('where', 'oliver')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "Top 10 unigrams, having highest frequency as in unigrams_freqDist:\n",
            "[(('the',), 1700), (('and',), 856), (('a',), 713), (('of',), 673), (('to',), 617), (('he',), 465), (('his',), 455), (('in',), 441), (('was',), 368), (('oliver',), 306)]\n",
            "\n",
            "\n",
            "Top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist:\n",
            "[(('oliver',), 306), (('said',), 212), (('mr',), 191), (('bumble',), 133), (('gentleman',), 108), (('old',), 89), (('sowerberry',), 80), (('boy',), 78), (('would',), 77), (('replied',), 74)]\n",
            "\n",
            "\n",
            "Top 10 bigrams, having highest frequency as in bigrams_freqDist:\n",
            "[(('of', 'the'), 162), (('in', 'the'), 127), (('mr', 'bumble'), 117), (('to', 'the'), 91), (('said', 'the'), 90), (('he', 'had'), 67), (('he', 'was'), 62), (('on', 'the'), 60), (('in', 'a'), 55), (('with', 'a'), 54)]\n",
            "\n",
            "\n",
            "Top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist:\n",
            "[(('mr', 'bumble'), 117), (('said', 'the'), 90), (('the', 'old'), 53), (('the', 'undertaker'), 44), (('old', 'gentleman'), 43), (('the', 'boy'), 39), (('the', 'jew'), 36), (('said', 'mr'), 35), (('the', 'gentleman'), 34), (('mr', 'sowerberry'), 33)]\n",
            "\n",
            "\n",
            "Top 10 trigrams, having highest frequency as in trigrams_freqDist:\n",
            "[(('the', 'old', 'gentleman'), 33), (('gentleman', 'in', 'the'), 22), (('the', 'gentleman', 'in'), 20), (('the', 'white', 'waistcoat'), 20), (('said', 'mr', 'bumble'), 19), (('in', 'the', 'white'), 18), (('said', 'the', 'undertaker'), 16), (('said', 'the', 'gentleman'), 14), (('said', 'the', 'jew'), 14), (('sir', 'replied', 'oliver'), 12)]\n",
            "\n",
            "\n",
            "Top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist:\n",
            "[(('the', 'old', 'gentleman'), 33), (('gentleman', 'in', 'the'), 22), (('the', 'gentleman', 'in'), 20), (('the', 'white', 'waistcoat'), 20), (('said', 'mr', 'bumble'), 19), (('in', 'the', 'white'), 18), (('said', 'the', 'undertaker'), 16), (('said', 'the', 'gentleman'), 14), (('said', 'the', 'jew'), 14), (('sir', 'replied', 'oliver'), 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO5ZiRxxW_BO"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqu8nVV7NREo"
      },
      "source": [
        "## **Next three words' Prediction using Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vnIM26b2WA"
      },
      "source": [
        "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
        "That is, pretend we saw each word one more time than we did.\n",
        "\n",
        "You have two tasks here.\n",
        "\n",
        "First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
        "\n",
        "As for example, for the string 'Raj has a' the answers can be as below: \n",
        "\n",
        "(1) Raj has a **beautiful red car**\n",
        "\n",
        "(2) Raj has a **charismatic magnetic personality**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAGB1_S8NThy"
      },
      "source": [
        "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
        "testSent2 = \"They made room for the stranger, but he sat down\"\n",
        "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLY1ymH-ZuJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f8b137-6876-43c2-efe3-6e6170b8c718"
      },
      "source": [
        "# *** Write code ***\n",
        "#print(bigrams_freqDist.elements)\n",
        "# for t,f in bigrams_freqDist.items():\n",
        "#        ##\n",
        "total_bigrams=len(bigrams)\n",
        "total_unique_bigrams=len(set(bigrams))\n",
        "\n",
        "total_trigrams=len(trigrams)\n",
        "total_unique_trigrams=len(set(trigrams))\n",
        "\n",
        "#print(total_bigrams)\n",
        "#print(total_unique_bigrams)\n",
        "\n",
        "bigrams_prob=[]\n",
        "trigrams_prob=[]\n",
        "for t,f in bigrams_freqDist.items():\n",
        "   temp=[t]\n",
        "   temp.append(f)\n",
        "   bigrams_prob.append(temp)\n",
        "for t,f in trigrams_freqDist.items():\n",
        "   temp=[t]\n",
        "   temp.append(f)\n",
        "   trigrams_prob.append(temp)\n",
        "\n",
        "for x in bigrams_prob:\n",
        "   x[-1]=(x[-1]+1)/(total_bigrams+total_unique_bigrams)\n",
        "for x in trigrams_prob:\n",
        "   x[-1]=(x[-1]+1)/(total_trigrams+total_unique_trigrams)\n",
        "\n",
        "bigrams_prob=sorted(bigrams_prob,key = lambda x:x[1], reverse= True)\n",
        "trigrams_prob=sorted(trigrams_prob,key = lambda x:x[1], reverse= True)\n",
        "\n",
        "# for x in trigrams_prob[:10]:\n",
        "#     print(x)\n",
        "\n",
        "# for x in bigrams_prob:\n",
        "#     if(x[0][0]==\"down\"):\n",
        "#         print(x[0][1],x[1])\n",
        "\n",
        "# for x in trigrams_prob:\n",
        "#      if x[0][:2]==('there','he'):\n",
        "#         print(x)\n",
        "def pred_three(text,val):\n",
        "   three=\"\"\n",
        "   if val==2:\n",
        "    temp=word_tokenize(text)[-1]\n",
        "    for i in range(3):\n",
        "        \n",
        "        for x in bigrams_prob:\n",
        "            #print(x[0][0])\n",
        "            if x[0][0]==temp:\n",
        "              three+=\" \"\n",
        "              three+=x[0][-1]\n",
        "              temp=x[0][-1]\n",
        "              #print(\"crazy\")\n",
        "              break\n",
        "   else: \n",
        "    temp=list(ngrams(word_tokenize(text),2))[-1]\n",
        "    #print(temp)\n",
        "    for i in range(3):\n",
        "        for x in trigrams_prob: \n",
        "           if x[0][:2]==temp:\n",
        "              #print(\"ishshshshsh\")\n",
        "              three+=\" \"\n",
        "              three+=x[0][-1]\n",
        "              temp=x[0][-2:]\n",
        "              #print(temp)\n",
        "              break            \n",
        "   return three\n",
        "\n",
        "#print(x)\n",
        "#print(x[0][:2])\n",
        "\n",
        "\n",
        "var=pred_three(testSent1,2)\n",
        "print(\"Bigram prediction: for testSent1: \",testSent1,var)\n",
        "var=pred_three(testSent1,3)\n",
        "print(\"Trigram prediction: for testSent1: \",testSent1,var)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "var=pred_three(testSent2,2)\n",
        "print(\"Bigram prediction: for testSent2: \",testSent2,var)\n",
        "var=pred_three(testSent2,3)\n",
        "print(\"Trigram prediction: for testSent2: \",testSent2,var)\n",
        "print(\"\\n\")\n",
        "\n",
        "var=pred_three(testSent3,2)\n",
        "print(\"Bigram prediction: for testSent3: \",testSent3,var)\n",
        "var=pred_three(testSent3,3)\n",
        "print(\"Trigram prediction: for testSent3: \",testSent3,var)\n",
        "print(\"\\n\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram prediction: for testSent1:  There was a sudden jerk, a terrific convulsion of the limbs; and there he  had been expected\n",
            "Trigram prediction: for testSent1:  There was a sudden jerk, a terrific convulsion of the limbs; and there he  sat down to\n",
            "\n",
            "\n",
            "Bigram prediction: for testSent2:  They made room for the stranger, but he sat down  the old gentleman\n",
            "Trigram prediction: for testSent2:  They made room for the stranger, but he sat down  to a branchworkhouse\n",
            "\n",
            "\n",
            "Bigram prediction: for testSent3:  The hungry and destitute situation of the infant orphan was duly reported by  the old gentleman\n",
            "Trigram prediction: for testSent3:  The hungry and destitute situation of the infant orphan was duly reported by  the side of\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY4PPpZVvtZ8"
      },
      "source": [
        "if ('hello','gello')==('hello','gello1'):\n",
        "   print('cheese')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfeaacTdO6h"
      },
      "source": [
        "Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\n",
        "\n",
        "Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMkW9hKecxK"
      },
      "source": [
        "  - - - - - - - - - -\n",
        "   **testSent1**: There was a sudden jerk, a terrific convulsion of the limbs; and there he<br/>\n",
        "    ActualSent:   There was a sudden jerk, a terrific convulsion of the limbs; and there he hung, with the <br/>\n",
        "   BigramPred: There was a sudden jerk, a terrific convulsion of the limbs; and there he  had been expected <br/>\n",
        "   TrigramPred:  There was a sudden jerk, a terrific convulsion of the limbs; and there he  sat down to\n",
        "\n",
        "   **testSent2**: They made room for the stranger, but he sat down<br/>\n",
        "    ActualSent:   They made room for the stranger, but he sat down in the furthest <br/>\n",
        "   BigramPred: They made room for the stranger, but he sat down  the old gentleman <br/>\n",
        "   TrigramPred:  They made room for the stranger, but he sat down  to a branchworkhouse\n",
        "\n",
        "   **testSent3**: The hungry and destitute situation of the infant orphan was duly reported by<br/>\n",
        "    ActualSent:   The hungry and destitute situation of the infant orphan was duly reported by the workhouse authorities <br/>\n",
        "   BigramPred: The hungry and destitute situation of the infant orphan was duly reported by  the old gentleman <br/>\n",
        "   TrigramPred:  The hungry and destitute situation of the infant orphan was duly reported by  the side of\n",
        "\n",
        "   <br/>\n",
        "   Neither Bigram nor Trigram smoothed models were able to predict the actual sentence.<br/>\n",
        "   testSent3 was present in the corpus,on the other hand testSent1 and testSent2 wasn't present in the corpus. Although, testSent3 was present in the corpus, but the smoothed model wasn't able to predict the actual sentence.\n",
        "   Because there was higher probability bigrams/trigrams was availble than the bigrams/trigrams in the actual sentence.\n",
        "\n",
        "   <br/>\n",
        "\n",
        "   - - - - - - - - - -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVBMcaAJXR9S"
      },
      "source": [
        "Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAPa1OVZX8uN"
      },
      "source": [
        "  - - - - - - - - - -\n",
        "\n",
        "  **Bigram model is better than Trigram model**.\n",
        "  The following code shows that for the given sentences\n",
        "  the probabilty of the prediction of the next three word is more in case of\n",
        "  Bigram model. As the probabilty is high, so the perplexity is small for Bigram model. We know the model having lower perplexity is better.\n",
        "  So the Bigram model is better than the Trigram model\n",
        "   - - - - - - - - - -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz1qH5zVOt85",
        "outputId": "b83304f9-e95b-4283-9e08-76e5a1075a42"
      },
      "source": [
        "def pred_prob(text,val):\n",
        "   prob=1\n",
        "   if val==2:\n",
        "    temp=word_tokenize(text)[-1]\n",
        "    for i in range(3):\n",
        "        \n",
        "        for x in bigrams_prob:\n",
        "            #print(x[0][0])\n",
        "            if x[0][0]==temp:\n",
        "              prob=prob*x[1]\n",
        "              temp=x[0][-1]\n",
        "              #print(\"crazy\")\n",
        "              break\n",
        "   else: \n",
        "    temp=list(ngrams(word_tokenize(text),2))[-1]\n",
        "    #print(temp)\n",
        "    for i in range(3):\n",
        "        for x in trigrams_prob: \n",
        "           if x[0][:2]==temp:\n",
        "              prob=prob*x[1]\n",
        "              temp=x[0][-2:]\n",
        "              #print(temp)\n",
        "              break            \n",
        "   return prob\n",
        "\n",
        "\n",
        "print(\"Bigram Prob\",\"               Trigram Prob\\n\")\n",
        "var1=pred_prob(testSent1,2)\n",
        "var2=pred_prob(testSent1,3)   \n",
        "print(var1,\"  \",var2)\n",
        "\n",
        "var1=pred_prob(testSent2,2)\n",
        "var2=pred_prob(testSent2,3)   \n",
        "print(var1,\"   \",var2)\n",
        "\n",
        "var1=pred_prob(testSent3,2)\n",
        "var2=pred_prob(testSent3,3)   \n",
        "print(var1,\"  \",var2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Prob                Trigram Prob\n",
            "\n",
            "1.1755147836525446e-10    1.9542578056334689e-13\n",
            "5.476515933251854e-10     1.302838537088979e-13\n",
            "1.7456394537240288e-09    5.428493904537412e-13\n"
          ]
        }
      ]
    }
  ]
}